---
title: "LDA"
author: "Elsa Marques"
date: "12/11/2017"
output: 
  html_document:    
    code_folding: hide
    fig_height: 4
    fig_width: 6
    highlight: espresso
    number_sections: yes
    toc: yes
    toc_float: yes 
---
 
## - Linear Discriminant analysis (LDA)

Linear discriminant analysis (LDA) is one of the methods that can be used for dimensionality reduction and cluster data. This method takes in account the overall data features in order to characterize a linear combination of features that allow the identification of sub-clusters in the data.

  - In order to perform LDA analysis:
      - The data needs to be scaled / standardized since the original data was not presenting the same variance range in all the variables; 
      - Test and train sets created to validade the model; 
    
    
- Starting by diving our data set into the train and test set

```{r, echo = FALSE}
# number of rows in the Boston dataset 
n <- nrow(Hobbies)
n

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- Hobbies[ind,]
train

# create test set 
test <- Hobbies[-ind,]
test

```

- Standardization of the data is needed in order to perform LDA analysis

```{r}

```


```{r, echo = FALSE}
library(dplyr)
library(MASS)
# linear discriminant analysis
lda.fit <- lda(N.Act ~ . , data = train)

# print the lda.fit object
lda.fit

# function for the lda biplot arrows
lda.arrows <- function(x, myscale = 2, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

```



```{r}
# target classes as numeric
classes <- train$N.Act


# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
```


```{r}

```


```{r, echo = FALSE}
# Center and standardize variables in the dataset
hobbies_scaled <- scale(hobbies_)

# Summary of the scaled variables
summary(hobbies_scaled)

# Check what is the class of the new scaled object
class(hobbies_scaled)

# change the object to data frame
hobbies_scaled <- as.data.frame(hobbies_scaled)
                               
# summary of the scaled crime rate
summary(hobbies_scaled$Prof)

# create a quantile vector of crim and print it
bins <- quantile(hobbies_scaled$Prof)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# check how boston_scaled looks like now
boston_scaled %>% group_by(crime)



library(dplyr)
# linear discriminant analysis
lda.fit <- lda(crime ~ . , data = train)

# print the lda.fit object
lda.fit

# function for the lda biplot arrows
lda.arrows <- function(x, myscale = 2, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

library(dplyr)
# save the correct classes from test data
correct_classes <- test$crime
correct_classes

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

# load MASS and Boston
library(MASS)
data("Boston")

# standardize Boston dataset
boston_scaled <- scale(Boston)

# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled, method ="manhattan")

# look at the summary of the distances
summary(dist_man)

# MASS, ggplot2 and Boston dataset are available
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

# Create a 3D plot of the columns of the matrix product
library(ggplot2)
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col  = train$crime)


plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col = train$km)
                               
```



