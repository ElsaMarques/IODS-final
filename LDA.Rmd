---
title: "LDA"
author: "Elsa Marques"
date: "12/11/2017"
output: 
  html_document:    
    code_folding: hide
    fig_height: 4
    fig_width: 6
    highlight: espresso
    number_sections: yes
    toc: yes
    toc_float: yes 
---
 
## - Linear Discriminant analysis (LDA)

Linear discriminant analysis (LDA) is one of the methods that can be used for dimensionality reduction and cluster data. This method takes in account the overall data features in order to characterize a linear combination of features that allow the identification of sub-clusters in the data.

  - In order to perform LDA analysis:
      - The data needs to be scaled / standardized since the original data was not presenting the same variance range in all the variables; 
      - Test and train sets created to validade the model; 
    
    
#### For this I selected the subgroup Artistic_Hobbies and Generic_Hobbies

```{r, echo = FALSE}
# number of rows in the Artistic_Hobbies dataset 
Artistic_Hobbies <- read.table("data/Artistic_Hobbies.csv", header = TRUE, sep = ",")
n <- nrow(Artistic_Hobbies)
n

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- Artistic_Hobbies[ind,]
train

# create test set 
test <- Artistic_Hobbies[-ind,]
test

```

- Standardization of the data is needed in order to perform LDA analysis


# Center and standardize variables in the dataset
Artistic_scaled <- scale(Artistic_Hobbies)

# Summary of the scaled variables
summary(Artistic_Hobbies_scaled)

# Check what is the class of the new scaled object
class(Hobbies_scaled)

# change the object to data frame
Hobbies_scaled <- as.data.frame(Hobbies_scaled)
                               



```{r, echo = FALSE}
library(dplyr)
library(GGally)
library(MASS)
# linear discriminant analysis
lda.fit <- lda(Prof ~ . , data = train)

# print the lda.fit object
lda.fit

# function for the lda biplot arrows
lda.arrows <- function(x, myscale = 2, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

```



```{r}
# plot the lda results
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 2)
```

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
```




# MASS, ggplot2 and Boston dataset are available
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

# Create a 3D plot of the columns of the matrix product
library(ggplot2)
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col  = train$crime)


plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col = train$km)
                               



